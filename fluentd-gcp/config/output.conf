# This match is placed before the all-matching output to provide metric
# exporter with a process start timestamp for correct exporting of
# cumulative metrics to Stackdriver.
<match process_start>
  @type prometheus

  <metric>
    type gauge
    name process_start_time_seconds
    desc Timestamp of the process start in seconds
    key process_start_timestamp
  </metric>
</match>

# This filter allows to count the number of log entries read by fluentd
# before they are processed by the output plugin. This in turn allows to
# monitor the number of log entries that were read but never sent, e.g.
# because of liveness probe removing buffer.
<filter **>
  @type prometheus
  <metric>
    type counter
    name logging_entry_count
    desc Total number of log entries generated by either application containers or system components
  </metric>
</filter>

# TODO(instrumentation): Reconsider this workaround later.
# Trim the entries which exceed slightly less than 100KB, to avoid
# dropping them. It is a necessity, because Stackdriver only supports
# entries that are up to 100KB in size.
<filter kubernetes.**>
  @type record_transformer
  enable_ruby true
  <record>
    log ${record['log'].length > 100000 ? "[Trimmed]#{record['log'][0..100000]}..." : record['log']}
  </record>
</filter>

# We use 2 output stanzas - one to handle the container logs and one to handle
# the node daemon logs, the latter of which explicitly sends its logs to the
# compute.googleapis.com service rather than container.googleapis.com to keep
# them separate since most users don't care about the node logs.
<match kubernetes.**>
  @type google_cloud

  # Try to detect JSON formatted log entries.
  detect_json true
  # Collect metrics in Prometheus registry about plugin activity.
  enable_monitoring true
  monitoring_type prometheus
  # Set the buffer type to file to improve the reliability and reduce the memory consumption
  buffer_type file
  buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer
  # Set queue_full action to block because we want to pause gracefully
  # in case of the off-the-limits load instead of throwing an exception
  buffer_queue_full_action block
  # Set the chunk limit conservatively to avoid exceeding the recommended
  # chunk size of 5MB per write request.
  buffer_chunk_limit 1M
  # Cap the combined memory usage of this buffer and the one below to # 1MiB/chunk * (6 + 2) chunks = 8 MiB
  buffer_queue_limit 6
  # Never wait more than 5 seconds before flushing logs in the non-error case.
  flush_interval 5s
  # Never wait longer than 30 seconds between retries.
  max_retry_wait 30
  # Disable the limit on the number of retries (retry forever).
  disable_retry_limit
  # Use multiple threads for processing.
  num_threads 2
</match>

# Keep a smaller buffer here since these logs are less important than the user's
# container logs.
#
<match **>
  @type copy

  <store>
    @type google_cloud

    detect_json true
    enable_monitoring true
    monitoring_type prometheus
    detect_subservice false
    buffer_type file
    buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer
    buffer_queue_full_action block
    buffer_chunk_limit 1M
    buffer_queue_limit 2
    flush_interval 5s
    max_retry_wait 30
    disable_retry_limit
    num_threads 2
  </store>

  <store>
    @type remote_syslog
    host logs7.papertrailapp.com
    port 45121
    severity info
    # program ${tag[0]}

    protocol tcp
    timeout 20
    timeout_exception true
    tls true
    ca_file /etc/papertrail-bundle.pem
    keep_alive true
    keep_alive_cnt 9
  </store>
</match>
